{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d1dda1",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check GPU & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1267534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import subprocess\n",
    "import psutil\n",
    "\n",
    "print(\"üîç Environment Check\")\n",
    "print(f\"Python: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f}GB\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "# RAM Check\n",
    "ram = psutil.virtual_memory()\n",
    "print(f\"RAM: {ram.total / 1e9:.2f}GB (Available: {ram.available / 1e9:.2f}GB)\")\n",
    "\n",
    "# NVIDIA SMI\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=index,name,driver_version,memory.total', '--format=csv,noheader'], \n",
    "                          capture_output=True, text=True)\n",
    "    print(f\"\\nüìä nvidia-smi Output:\\n{result.stdout}\")\n",
    "except Exception as e:\n",
    "    print(f\"nvidia-smi not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e70b60c",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9830490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q transformers torch bitsandbytes accelerate peft llama-cpp-python fastapi uvicorn pydantic redis aiohttp psutil\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27956d93",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Download & Load Qwen-2.5-Coder (Q4_K_M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522e088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch.cuda\n",
    "\n",
    "# Check VRAM before loading\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "print(\"üì• Loading Qwen-2.5-Coder 7B (Q4_K_M)...\")\n",
    "\n",
    "# Configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Model configuration\n",
    "MODEL_ID = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    print(f\"‚úÖ Tokenizer loaded: {MODEL_ID}\")\n",
    "    \n",
    "    # Load model with quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    print(f\"‚úÖ Model loaded: {MODEL_ID}\")\n",
    "    \n",
    "    # Check VRAM usage\n",
    "    if torch.cuda.is_available():\n",
    "        vram_used = torch.cuda.memory_allocated() / 1e9\n",
    "        vram_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"\\nüìä VRAM Usage: {vram_used:.2f}GB / {vram_total:.2f}GB\")\n",
    "        print(f\"Memory Peak: {torch.cuda.max_memory_allocated() / 1e9:.2f}GB\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"\\nüí° Try:\") \n",
    "    print(\"1. Restart the runtime: Runtime ‚Üí Restart runtime\")\n",
    "    print(\"2. Use a smaller model: Qwen/Qwen2.5-Coder-1.5B-Instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89b489d",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Test Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd25b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_tokens: int = 128) -> str:\n",
    "    \"\"\"Generate response using Qwen-2.5-Coder.\"\"\"\n",
    "    try:\n",
    "        # Prepare input\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.2,\n",
    "                top_p=0.9,\n",
    "                top_k=40,\n",
    "                repetition_penalty=1.05,\n",
    "                do_sample=True,\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"# Python function to calculate fibonacci\\ndef fibonacci\",\n",
    "    \"# Refactor this code:\\nfor i in range(len(list)):\",\n",
    "    \"# Security audit: find issues in this SQL query:\",\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing Model Inference\\n\")\n",
    "for i, prompt in enumerate(test_prompts[:1], 1):  # Test first prompt only to save tokens\n",
    "    print(f\"Test {i}:\")\n",
    "    print(f\"Prompt: {prompt}...\")\n",
    "    response = generate_response(prompt, max_tokens=64)\n",
    "    print(f\"Response: {response[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a78d1fc",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Setup FastAPI Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from typing import Optional\n",
    "import uvicorn\n",
    "import asyncio\n",
    "from threading import Thread\n",
    "\n",
    "# Define request/response models\n",
    "class GenerateRequest(BaseModel):\n",
    "    prompt: str\n",
    "    task: str = \"general\"\n",
    "    max_tokens: int = 256\n",
    "    temperature: float = 0.2\n",
    "\n",
    "class GenerateResponse(BaseModel):\n",
    "    response: str\n",
    "    task: str\n",
    "    tokens: int\n",
    "    model: str\n",
    "\n",
    "# Create FastAPI app\n",
    "app = FastAPI(\n",
    "    title=\"Colab AI Coder API\",\n",
    "    description=\"Assistant IA bas√© sur Qwen-2.5-Coder 7B\",\n",
    "    version=\"0.1.0\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ FastAPI app created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96f46ce",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Define API Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005e3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    \"\"\"Health check endpoint.\"\"\"\n",
    "    return {\"status\": \"healthy\", \"model\": \"qwen2.5-coder:7b\"}\n",
    "\n",
    "@app.post(\"/api/v1/assistant/generate\", response_model=GenerateResponse)\n",
    "async def generate(request: GenerateRequest):\n",
    "    \"\"\"Generate code using Qwen-2.5-Coder.\"\"\"\n",
    "    try:\n",
    "        response = generate_response(request.prompt, max_tokens=request.max_tokens)\n",
    "        return GenerateResponse(\n",
    "            response=response,\n",
    "            task=request.task,\n",
    "            tokens=len(response.split()),\n",
    "            model=\"qwen2.5-coder:7b-q4_k_m\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.get(\"/api/v1/models/current\")\n",
    "async def get_current_model():\n",
    "    \"\"\"Get current model info.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        vram_used = torch.cuda.memory_allocated() / 1e9\n",
    "        vram_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    else:\n",
    "        vram_used = vram_total = 0\n",
    "    \n",
    "    return {\n",
    "        \"name\": \"qwen2.5-coder:7b-q4_k_m\",\n",
    "        \"size\": \"5.2GB\",\n",
    "        \"vram_usage\": f\"{vram_used:.2f}GB / {vram_total:.2f}GB\",\n",
    "        \"status\": \"ready\"\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Routes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b331841",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Start FastAPI Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Get ngrok URL for public access\n",
    "try:\n",
    "    from google.colab import ngrok\n",
    "    \n",
    "    # Start server in background\n",
    "    def run_server():\n",
    "        uvicorn.run(\n",
    "            app,\n",
    "            host=\"127.0.0.1\",\n",
    "            port=8000,\n",
    "            log_level=\"info\"\n",
    "        )\n",
    "    \n",
    "    # Run in thread\n",
    "    server_thread = Thread(target=run_server, daemon=True)\n",
    "    server_thread.start()\n",
    "    \n",
    "    print(\"‚úÖ FastAPI server started on http://127.0.0.1:8000\")\n",
    "    \n",
    "    # Setup ngrok tunnel\n",
    "    public_url = ngrok.connect(8000)\n",
    "    print(f\"üåê Public URL (ngrok): {public_url}\")\n",
    "    print(f\"üìö API Docs: {public_url}/docs\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è ngrok not available in this environment: {e}\")\n",
    "    print(\"Local API will be available on: http://127.0.0.1:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7009582",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Example: Use Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b566829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Wait for server to start\n",
    "time.sleep(2)\n",
    "\n",
    "# Example 1: Generate code\n",
    "print(\"\\nüî® Example 1: Generate Python Function\\n\")\n",
    "prompt = \"\"\"# Python function to calculate factorial\n",
    "# Input: n (integer)\n",
    "# Output: factorial of n\n",
    "def factorial(n):\"\"\"\n",
    "\n",
    "response = generate_response(prompt, max_tokens=100)\n",
    "print(\"Generated Code:\")\n",
    "print(response)\n",
    "\n",
    "# Example 2: Refactor\n",
    "print(\"\\n\\nüîß Example 2: Refactor Code\\n\")\n",
    "refactor_prompt = \"\"\"# Refactor this code for better performance and readability:\n",
    "for i in range(len(my_list)):\n",
    "    for j in range(len(my_list)):\n",
    "        if my_list[i] == my_list[j]:\n",
    "            print(my_list[i])\"\"\"\n",
    "\n",
    "response = generate_response(refactor_prompt, max_tokens=100)\n",
    "print(\"Refactored Code:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c0cb21",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Connect from VS Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78840b3",
   "metadata": {},
   "source": [
    "### Steps to connect from VS Code:\n",
    "\n",
    "1. **Get the API URL:**\n",
    "   - Copy the public URL from the cell above (if using ngrok)\n",
    "   - Or use: `http://127.0.0.1:8000` for local testing\n",
    "\n",
    "2. **Install Colab AI Coder Extension** (coming soon):\n",
    "   - Open VS Code\n",
    "   - Go to Extensions (Ctrl+Shift+X)\n",
    "   - Search for \"Colab AI Coder\"\n",
    "   - Click Install\n",
    "\n",
    "3. **Configure extension:**\n",
    "   - Open Command Palette (Ctrl+Shift+P)\n",
    "   - Type \"Colab AI Coder: Configure API\"\n",
    "   - Paste the API URL\n",
    "\n",
    "4. **Use the assistant:**\n",
    "   - Right-click on code\n",
    "   - Select \"Generate\", \"Refactor\", \"Debug\", or \"Audit\"\n",
    "   - View results in sidebar\n",
    "\n",
    "### API Endpoints:\n",
    "\n",
    "```bash\n",
    "# Health check\n",
    "curl http://API_URL/health\n",
    "\n",
    "# Generate code\n",
    "curl -X POST http://API_URL/api/v1/assistant/generate \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"prompt\": \"def hello\", \"task\": \"generate\", \"max_tokens\": 100}'\n",
    "\n",
    "# Get current model\n",
    "curl http://API_URL/api/v1/models/current\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b413fc62",
   "metadata": {},
   "source": [
    "## üîü Monitor Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ee3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import torch\n",
    "\n",
    "def monitor_resources():\n",
    "    \"\"\"Monitor system resources.\"\"\"\n",
    "    print(\"\\nüìä Resource Monitor\\n\")\n",
    "    \n",
    "    # CPU\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)\n",
    "    print(f\"CPU Usage: {cpu_percent}%\")\n",
    "    \n",
    "    # Memory\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(f\"RAM Usage: {ram.percent}% ({ram.used / 1e9:.2f}GB / {ram.total / 1e9:.2f}GB)\")\n",
    "    \n",
    "    # GPU\n",
    "    if torch.cuda.is_available():\n",
    "        vram_used = torch.cuda.memory_allocated() / 1e9\n",
    "        vram_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        print(f\"GPU VRAM: {vram_used:.2f}GB / {vram_total:.2f}GB ({vram_used/vram_total*100:.1f}%)\")\n",
    "        print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Disk\n",
    "    disk = psutil.disk_usage('/')\n",
    "    print(f\"Disk Usage: {disk.percent}% ({disk.used / 1e9:.2f}GB / {disk.total / 1e9:.2f}GB)\")\n",
    "\n",
    "monitor_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1f07a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Setup Complete!\n",
    "\n",
    "Your Colab AI Coder environment is ready. You can now:\n",
    "\n",
    "- ü§ñ Generate code using Qwen-2.5-Coder\n",
    "- üîß Refactor and debug code\n",
    "- üîí Audit code for security issues\n",
    "- üìö Access the API from VS Code\n",
    "- üìä Monitor VRAM and performance\n",
    "\n",
    "Keep this cell running to maintain the API server.\n",
    "\n",
    "For more info, visit: [Colab AI Coder GitHub](https://github.com/yourusername/colab-ai-coder)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
